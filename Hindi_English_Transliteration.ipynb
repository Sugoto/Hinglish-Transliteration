{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sugoto/Indic-Language-NLP/blob/main/Hindi_English_Transliteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWemH6XxUFEp"
      },
      "source": [
        "###Import all the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-preprocessing"
      ],
      "metadata": {
        "id": "pkIqUFTQkluu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9P0ZlBKEkCdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC6QlOfdUEx1"
      },
      "source": [
        "from os.path import isfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNOFy1fFUuJp"
      },
      "source": [
        "###Choose Translate direction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP2JBLp9Uznq"
      },
      "source": [
        "#Set htoe as true for translation from Hindi to Hnglish; \n",
        "#Set htoe as false for English to Hindi translation\n",
        "htoe = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-4tRIHdUNN8"
      },
      "source": [
        "###Load the Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSidzxZZOC_o"
      },
      "source": [
        "if htoe:\n",
        "    tokenizer_one = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/tokenizer_hi\",\"rb\"))\n",
        "    tokenizer_two = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/tokenizer_en\",\"rb\"))\n",
        "else:\n",
        "    tokenizer_one = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/tokenizer_en\",\"rb\"))\n",
        "    tokenizer_two = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/tokenizer_hi\",\"rb\"))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0NwdDczUrlk"
      },
      "source": [
        "###Create the dataset - **For training purposes only**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweak the dataset params according to your needs\n",
        "\n",
        "# MAX_LENGTH = 64 \n",
        "# BUFFER_SIZE = 1000\n",
        "# BATCH_SIZE = 100"
      ],
      "metadata": {
        "id": "O2xWmjlLUOZt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read the dataset objects\n",
        "\n",
        "# if htoe:\n",
        "#     raw_data_one = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/raw_data_hi\",\"rb\"))\n",
        "#     raw_data_two = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/raw_data_en\",\"rb\"))\n",
        "# else:\n",
        "#     raw_data_one = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/raw_data_en\",\"rb\"))\n",
        "#     raw_data_two = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/NLP/raw_data_hi\",\"rb\"))\n",
        "\n",
        "# print(\"Dataset size: {}\".format(len(raw_data_one)))"
      ],
      "metadata": {
        "id": "tDHFAYR_UPcx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for data preprocessing\n",
        "\n",
        "# def encode(lang1, lang2):\n",
        "#   lang1 = [len(tokenizer_one.word_index)] + tokenizer_one.texts_to_sequences(\n",
        "#       [lang1.numpy().decode(\"utf-8\")])[0] + [len(tokenizer_one.word_index)+1]\n",
        "\n",
        "#   lang2 = [len(tokenizer_two.word_index)] + tokenizer_two.texts_to_sequences(\n",
        "#       [lang2.numpy().decode(\"utf-8\")])[0] + [len(tokenizer_two.word_index)+1]\n",
        "  \n",
        "#   return lang1, lang2\n",
        "\n",
        "# def tf_encode(lang1, lang2):\n",
        "#   result_one, result_two = tf.py_function(\n",
        "#       encode, [lang1, lang2], [tf.int64, tf.int64])\n",
        "#   result_one.set_shape([None])\n",
        "#   result_two.set_shape([None])\n",
        "\n",
        "#   return result_one, result_two\n",
        "\n",
        "# def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "#   return tf.logical_and(tf.size(x) <= max_length,\n",
        "#                         tf.size(y) <= max_length)"
      ],
      "metadata": {
        "id": "ODUOLKWCURCO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain tf-data object from the dataset\n",
        "\n",
        "# train_examples = tf.data.Dataset.from_tensor_slices((raw_data_one, raw_data_two)) \n",
        "\n",
        "# train_preprocessed = (\n",
        "#     train_examples \n",
        "#     .map(tf_encode)\n",
        "#     .filter(filter_max_length)\n",
        "#     .cache()\n",
        "#     .shuffle(BUFFER_SIZE))\n",
        "\n",
        "# train_dataset = (train_preprocessed\n",
        "#                  .padded_batch(BATCH_SIZE)\n",
        "#                  .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "wuHljYC6UUCF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6IkKAYnVUrb"
      },
      "source": [
        "###Create the word embedding matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3iTs5X1VaS8"
      },
      "source": [
        "# Define the vocab sizes\n",
        "\n",
        "input_vocab_size = len(tokenizer_one.word_index) + 2\n",
        "target_vocab_size = len(tokenizer_two.word_index) + 2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdImo-P8VdNQ"
      },
      "source": [
        "# Load the pretrained embeddings\n",
        "\n",
        "words_en, embeddings_en = pickle.load(\n",
        "    open('/content/drive/MyDrive/Colab Notebooks/NLP/polyglot-en.pkl', 'rb'), encoding='latin1')\n",
        "words_hi, embeddings_hi = pickle.load(\n",
        "    open('/content/drive/MyDrive/Colab Notebooks/NLP/polyglot-hi.pkl', 'rb'), encoding='latin1')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding layer for English"
      ],
      "metadata": {
        "id": "ssf8My75Fnul"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5OqV9X-Vfb7"
      },
      "source": [
        "EMBEDDING_DIM = 64\n",
        "\n",
        "word_index_en = tokenizer_two.word_index if htoe else tokenizer_one.word_index\n",
        "embeddings_index_en = {word.lower(): embedding for word, embedding in zip(words_en, embeddings_en)}\n",
        "\n",
        "embedding_matrix_en = np.zeros((target_vocab_size if htoe else input_vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for i, (word, index) in enumerate(word_index_en.items()):\n",
        "    embedding_matrix_en[index] = embeddings_index_en[word.lower()] if word.lower() in embeddings_index_en else np.zeros(EMBEDDING_DIM)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding layer for Hindi"
      ],
      "metadata": {
        "id": "VeDEUCwbFtNb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrYgRR3LViqd"
      },
      "source": [
        "EMBEDDING_DIM = 64\n",
        "\n",
        "word_index_hi = tokenizer_one.word_index if htoe else tokenizer_two.word_index\n",
        "embeddings_index_hi = {word: embedding for word, embedding in zip(words_hi, embeddings_hi)}\n",
        "\n",
        "embedding_matrix_shape = (input_vocab_size, EMBEDDING_DIM) if htoe else (target_vocab_size, EMBEDDING_DIM)\n",
        "embedding_matrix_hi = np.zeros(embedding_matrix_shape)\n",
        "\n",
        "for word, i in word_index_hi.items():\n",
        "    embedding_vector = embeddings_index_hi.get(word)\n",
        "    embedding_matrix_hi[i] = embedding_vector if embedding_vector is not None else np.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBApmP9bYiL1"
      },
      "source": [
        "embedding_matrix_one = embedding_matrix_hi if htoe else embedding_matrix_en\n",
        "embedding_matrix_two = embedding_matrix_en if htoe else embedding_matrix_hi"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjKd29iJY52Y"
      },
      "source": [
        "###Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn7ARpFxY049"
      },
      "source": [
        "# Set the model Hyperparams\n",
        "\n",
        "num_layers = 6\n",
        "d_model = 64\n",
        "dff = 2048\n",
        "num_heads = 32\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlkn6ckKY74t"
      },
      "source": [
        "# Utility functions for the model\n",
        "\n",
        "def positional_encoding(max_pos, d_model, scaling_factor=10000):\n",
        "    positions = np.arange(max_pos)[:, np.newaxis]\n",
        "    angles = positions / np.power(scaling_factor, \n",
        "                                  2 * np.arange(d_model)[np.newaxis, :] / d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    return tf.cast(angles[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  \n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  \n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  \n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  \n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "\n",
        "    # (batch_size, seq_len, dff)\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-1mW4d_Y-Cu"
      },
      "source": [
        "# Define the Model layers\n",
        "\n",
        "# Multihead Attention keras layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    # (batch_size, seq_len, d_model)\n",
        "    q = self.wq(q)  \n",
        "    k = self.wk(k)\n",
        "    v = self.wv(v)\n",
        "    \n",
        "    # (batch_size, num_heads, seq_len_q, depth)\n",
        "    q = self.split_heads(q, batch_size)\n",
        "\n",
        "    # (batch_size, num_heads, seq_len_k, depth)  \n",
        "    k = self.split_heads(k, batch_size)\n",
        "\n",
        "    # (batch_size, num_heads, seq_len_v, depth)\n",
        "    v = self.split_heads(v, batch_size)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    # (batch_size, seq_len_q, num_heads, depth)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "\n",
        "    # (batch_size, seq_len_q, d_model)\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  \n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights\n",
        "\n",
        "# Encoder keras Layer\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  \n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    out1 = self.layernorm1(x + attn_output)  \n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.ffn(out1)  \n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  \n",
        "    \n",
        "    return out2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "        input_vocab_size, \n",
        "        d_model, \n",
        "        weights = [embedding_matrix_one], \n",
        "        trainable = False)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    x = self.embedding(x)  \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    return x  \n",
        "\n",
        "# Decoder keras layer\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  \n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  \n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    out2 = self.layernorm2(attn2 + out1)  \n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.ffn(out2)  \n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  \n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "        target_vocab_size, \n",
        "        d_model, \n",
        "        weights = [embedding_matrix_two], \n",
        "        trainable=False)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    x = self.embedding(x) \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKNtu-PfY_4R"
      },
      "source": [
        "# Define the Model\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    # (batch_size, inp_seq_len, d_model)\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  \n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    final_output = self.final_layer(dec_output)  \n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWXpDKiJZB5p"
      },
      "source": [
        "# Define the Learning Rate\n",
        "\n",
        "# Custom Learning rate scheduler\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "# create the optimizer object with the custom learning rate\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAYh1J_hZEqG"
      },
      "source": [
        "# Define the loss for the model\n",
        "\n",
        "# Create the required type of loss\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, \n",
        "    reduction='none')\n",
        "\n",
        "# Define how loss is calculated\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "# Create the loss and accuracy objects\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(\n",
        "    name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGR2w5ALZGlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf0361b-eab4-4ee3-c37b-ac04814e9d08"
      },
      "source": [
        "# Create the model object with the necessary params\n",
        "\n",
        "# Unzip the weights\n",
        "\n",
        "!unzip -o \"/content/drive/MyDrive/Colab Notebooks/NLP/transformer_htoe.weights.zip\"\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "# Load any pre-trained weights (Comment out if retraining from scratch)\n",
        "\n",
        "if htoe:\n",
        "    transformer.load_weights('transformer_htoe.weights')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/NLP/transformer_htoe.weights.zip\n",
            "  inflating: transformer_htoe.weights.data-00000-of-00002  \n",
            "  inflating: transformer_htoe.weights.data-00001-of-00002  \n",
            "  inflating: transformer_htoe.weights.index  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk56C0B1j4rI"
      },
      "source": [
        "###Train the model - Not needed for predictions only"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define training epochs\n",
        "\n",
        "# EPOCHS = 2"
      ],
      "metadata": {
        "id": "CnePk0dIMkGe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the training step function\n",
        "\n",
        "# train_step_signature = [\n",
        "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "# ]\n",
        "\n",
        "# @tf.function(input_signature=train_step_signature)\n",
        "# def train_step(inp, tar):\n",
        "#   tar_inp = tar[:, :-1]\n",
        "#   tar_real = tar[:, 1:]\n",
        "  \n",
        "#   enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "#   with tf.GradientTape() as tape:\n",
        "#     predictions, _ = transformer(inp, tar_inp, \n",
        "#                                  True, \n",
        "#                                  enc_padding_mask, \n",
        "#                                  combined_mask, \n",
        "#                                  dec_padding_mask)\n",
        "#     loss = loss_function(tar_real, predictions)\n",
        "\n",
        "#   gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "#   optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "#   train_loss(loss)\n",
        "#   train_accuracy(tar_real, predictions)"
      ],
      "metadata": {
        "id": "RIoz6VjXMlZ-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for epoch in range(EPOCHS):\n",
        "#   start = time.time()\n",
        "  \n",
        "#   train_loss.reset_states()\n",
        "#   train_accuracy.reset_states()\n",
        "  \n",
        "#   for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "#     train_step(inp, tar)\n",
        "    \n",
        "#     if batch % 50 == 0:\n",
        "#       print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "#           epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "#   transformer.save_weights('transformer_{}.weights'.format(epoch + 1))\n",
        "#   print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "#                                                 train_loss.result(), \n",
        "#                                                 train_accuracy.result()))\n",
        "\n",
        "#   print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "OyL_o4PeMqF2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbZ1h08-kTuL"
      },
      "source": [
        "###Predict sentences using the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WAUMu_XkUFf"
      },
      "source": [
        "# Utility functions for prediction\n",
        "\n",
        "MAX_LENGTH = 64 \n",
        "\n",
        "def preprocess_string(s):\n",
        "  \n",
        "    if htoe:\n",
        "        s = re.sub(r'[a-zA-Z]', '', s)\n",
        "    s = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", s)\n",
        "    s = re.sub(r'([!.?।])', r' \\1', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    \n",
        "    return s\n",
        "\n",
        "def evaluate(inp_sentence):\n",
        "  start_token = [len(tokenizer_one.word_index)]\n",
        "  end_token = [len(tokenizer_one.word_index) + 1]\n",
        "  \n",
        "  inp_sentence = start_token + tokenizer_one.texts_to_sequences(\n",
        "      [inp_sentence])[0] + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  decoder_input = [len(tokenizer_two.word_index)]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(64):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == len(tokenizer_two.word_index)+1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "def translate(sentence):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  predicted_sentence = tokenizer_two.sequences_to_texts(\n",
        "      [[i.numpy()] for i in result if i < len(tokenizer_two.word_index)])  \n",
        "\n",
        "  return ' '.join(predicted_sentence)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9h-euuwlTUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce91dd0-63da-4c0a-d1ea-167ae01cd1be"
      },
      "source": [
        "#Enter input sentence\n",
        "inp_str = \"मेरा नाम क्या है\"\n",
        "\n",
        "print(translate(preprocess_string(inp_str)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is my name\n"
          ]
        }
      ]
    }
  ]
}